%% LyX 1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english, 12pt]{article}
\usepackage{times}
%\usepackage{algorithm2e}
\usepackage{url}
\usepackage{bbm}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\geometry{verbose,letterpaper,tmargin=2cm,bmargin=2cm,lmargin=1.5cm,rmargin=1.5cm}
\usepackage{rotating}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{setspace}
\usepackage{lineno}
\usepackage{hyperref}
\usepackage{bbm}
\usepackage{makecell}

\renewcommand{\arraystretch}{1.3}

\usepackage{xr}
\externaldocument{paper-misspec-supp}

%\linenumbers
%\doublespacing
\onehalfspacing
%\usepackage[authoryear]{natbib}
\usepackage{natbib} \bibpunct{(}{)}{;}{author-year}{}{,}

%Pour les rajouts
\usepackage{color}
\definecolor{trustcolor}{rgb}{0,0,1}

\usepackage{dsfont}
\usepackage[warn]{textcomp}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{graphicx}
\graphicspath{{../figures/}}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\usepackage{algorithm}
\usepackage{algpseudocode}

\let\tabbeg\tabular
\let\tabend\endtabular
\renewenvironment{tabular}{\begin{adjustbox}{max width=0.9\textwidth}\tabbeg}{\tabend\end{adjustbox}}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Bold symbol macro for standard LaTeX users
%\newcommand{\boldsymbol}[1]{\mbox{\boldmath $#1$}}

%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

\usepackage{babel}
\makeatother


\begin{document}


\title{Identifying and correcting multiple sources of misspecification\\in GWAS summary statistics for prediction}
\author{Florian Priv\'e,$^{\text{1,}*}$ Julyan Arbel,$^{\text{2}}$ Timothy S. H. Mak,$^{\text{3}}$ Hugues Aschard,$^{\text{4,5}}$ and Bjarni J. Vilhj\'almsson$^{\text{1,6}}$}

\date{~ }
\maketitle

\noindent$^{\text{\sf 1}}$National Centre for Register-Based Research, Aarhus University, Aarhus, 8210, Denmark. \\
\noindent$^{\text{\sf 2}}$Univ.\ Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, Grenoble, 38000, France. \\
\noindent$^{\text{\sf 3}}$Fano Labs, Hong Kong. \\
\noindent$^{\text{\sf 4}}$Department of Computational Biology, Institut Pasteur, Paris, 75015, France. \\
\noindent$^{\text{\sf 5}}$Program in Genetic Epidemiology and Statistical Genetics, Harvard T.H. Chan School of Public Health, Boston, MA, 02115, USA. \\
\noindent$^{\text{\sf 6}}$Bioinformatics Research Centre, Aarhus University, Aarhus, 8000, Denmark. \\
\noindent$^\ast$To whom correspondence should be addressed.\\

\noindent Contact: \url{florian.prive.21@gmail.com}

\vspace*{8em}

\abstract{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Results}

\subsection*{Misspecification of GWAS sample sizes}

We design simulations where variants have different GWAS sample sizes, which is often the case when meta-analyzing GWAS from multiple cohorts without the same genome coverage.
Using 40,000 variants from chromosome 22 (Methods), we simulate quantitative phenotypes assuming an heritability of 20\% and 2000 causal variants.
For half of the variants, we use 100\% of 300,000 individuals for GWAS, but use only 80\% for one quarter of them and 60\% for the remaining quarter. 
We then run C+T, LDpred2-inf, LDpred2(-grid), LDpred2-auto, lassosum, lassosum2, PRS-CS and SBayesR by using either the true per-variant GWAS sample size, or the total sample size. 
We do not show results for SBayesR because it always diverged and for PRS-CS because the overlap with the LD reference they provide is too small.
Averaged over 10 simulations, when providing true per-variant GWAS sample sizes, squared correlations (in \%) between the polygenic scores and the simulated phenotypes are of [TODO: COMPLETE] (Figure \ref{fig:simu-misN}).
Note that C+T does not use this sample size information.
When using the total GWAS sample size instead of the per-variant sample sizes, predictive performance slightly decreases [TODO: REPLACE NUMBERS] from 17.5 to 17.4 for lassosum, from 18.0 to 17.5 for lassosum2, but dramatically decreases for LDpred2 with new values of 11.5 for LDpred2-grid, 5.7 for LDpred2-auto and 6.8 for LDpred2-inf (Figure \ref{fig:simu-misN}).
Therefore, this extreme simulation scenario shows that LDpred2 can be sensitive to GWAS sample size misspecification, whereas lassosum (and lassosum2) seems little affected by this.

\begin{figure}[h]
	\centerline{\includegraphics[width=0.95\textwidth]{simu-misN}}
	\caption{Results for the simulations with sample size misspecification. The GWAS sample size is ``true'' when providing the true per-variant sample size, ``max'' when providing instead the maximum sample size as a unique value to be used for all variants,  ``imputed'' (see Methods in main text), or ``any'' when the method does not use this information (the case for C+T).}
	\label{fig:simu-misN}
\end{figure}

We then conduct further investigations to explain results of figure \ref{fig:simu-misN}.
First, the reason why results for LDpred2-auto are the same as with LDpred2-inf is because it always converges to an infinitesimal model (p = 1) in these simulations.
Second, for lassosum2, results for a grid of parameters (over $\lambda$ and $\delta$) and quite smooth compared to LDpred2 (Figures \ref{fig:lassosum2-misN} and \ref{fig:ldpred2-misN}). Results slightly improve globally for both lassosum2 and LDpred2 when using imputed instead of maximum GWAS sample sizes.
Finally, in these simulations with misspecified sample sizes, it seems highly beneficial to use a small value for the SNP heritability hyper-parameter in LDpred2, e.g.\ a value of 0.02 or even 0.002 when the true value is 0.2 (Figure \ref{fig:ldpred2-misN}). 
We recall that using a small value for this hyper-parameter induces a larger regularization (shrinkage) on the effect sizes, and this benefits LDpred2 even when using imputed sample sizes. 

[NEED TO TALK ABOUT THE LD TRANSFO SOMEWHERE + NEW METHODS]


\subsection*{When using allele dosages from imputation}

When using the expected genotypes from imputation (dosages), their standard deviations are usually lower than the expected value under Hardy-Weinberg equilibrium (cf.\ Methods section ``Standard deviations and imputation measures'').
In simulations (cf.\ Methods section ``Data for simulations''), we show that $\text{sd}(G_j)^{true} \approx \text{sd}(G_j)^{imp} / \sqrt{\text{INFO}_j}$, $\text{se}(\hat{\beta}_j)^{true} \approx \text{se}(\hat{\beta}_j)^{imp} \cdot \sqrt{\text{INFO}_j}$, and $\hat{\beta}_j^{true} \approx \hat{\beta}_j^{imp} \cdot \sqrt{\text{INFO}_j}$ (Figures \ref{fig:compare-sd}, \ref{fig:compare-beta}, and \ref{fig:compare-beta-se}).
This is the first correction of summary statistics we try in the simulations below. 
Note that we recompute INFO scores for the subset of 362,307 European individuals used in this paper as they can differ substantially from the ones reported by the UK Biobank for the whole data (Figure \ref{fig:compare-info}).
As a second option, instead of using dosages to perform the GWAS, it has been argued that using multiple imputation (MI) would be more appropriate \cite[]{Palmer2016}.
In simulations, we show that $Z_j^{MI} \approx Z_j^{imp} \cdot \text{INFO}_j$ and $\hat{\beta}_j^{MI} \approx \hat{\beta}_j^{imp} \cdot \text{INFO}_j$ (Figure \ref{fig:compare-mi}).
This is the second correction of summary statistics we try in the simulations below, along with $n_j = N \cdot \text{INFO}_j$. 
Finally, we try an in-between solution as a third correction, using $\hat{\beta}_j^{imp} \cdot \text{INFO}_j$, $\text{se}(\hat{\beta}_j)^{imp} \cdot \sqrt{\text{INFO}_j}$, and $N \cdot \text{INFO}_j$.

Using 40,000 variants from chromosome 22, we simulate quantitative phenotypes assuming an heritability of 20\% and 2000 causal variants using the ``true'' dataset (cf.\ Methods section ``Data for simulations'').
We run GWAS on the dosage dataset and use these summary statistics to run LDpred2 and lassosum2 with either no correction of the summary statistics, or with one of the three described above.
The LD reference used by LDpred2 and lassosum2 is computed from the validation set using the ``true'' dataset.
For lassosum2 and LDpred2 which tune parameters using validation set and can possibly use strong regularization, the two corrections just slighly improve predictive performance in these simulations (Figure \ref{fig:simu-info}).
All three correction improve predictive performance (Figure \ref{fig:simu-info}), [TODO: FINISH WHEN INTRODUCED TRANSFO + NEW METHODS].
In the real data applications hereinafter, we choose to use the first correction, ``sqrt\_info'', which is simple because it is equivalent to postprocessing PGS effects by multiplying them by $\sqrt{\text{INFO}_j}$.

\begin{figure}[h]
	\centerline{\includegraphics[width=0.95\textwidth]{simu-info}}
	\caption{Results for the simulations using dosage data. Correction ``sqrt\_info'' corresponds to using $\hat{\beta}_j^{imp} \cdot \sqrt{\text{INFO}_j}$ and $\text{se}(\hat{\beta}_j)^{imp} \cdot \sqrt{\text{INFO}_j}$.
	Correction ``info'' corresponds to using $\hat{\beta}_j^{imp} \cdot \text{INFO}_j$  and $N \cdot \text{INFO}_j$.}
	\label{fig:simu-info}
\end{figure}

\subsection*{Application to breast cancer summary statistics}

[TALK ABOUT THE SAMPLE SIZES BEFORE/AFTER QC?]

Breast cancer summary statistics are interesting because they include results from two mega analyses \cite[]{michailidou2013large,michailidou2015genome,michailidou2017association}, which means there is some larger precision in the parameters reported, such as the INFO scores and the sample sizes.
INFO scores for the OncoArray summary statistics (after having restricted to HapMap3 variants) are globally very good (Figure \ref{fig:hist-info-brca}), but the ones from iCOGS are a bit worse (Figure \ref{fig:hist-info-brca2}), probably because the chip used included around 200K variants only, compared to more than 500K variants for the OncoArray.
For the OncoArray summary statistics, we first compare the standard deviations inferred from the reported allele frequencies (i.e.\ $\sqrt{2 f (1 - f)}$ where $f$ is the allele frequency, and denoted as sd\_af) versus the ones inferred from the GWAS summary statistics (Equation \eqref{eq:approx-sd-log}, and denoted as sd\_ss).
When coloring by INFO scores, we see a clear trend with sd\_ss being lower and lower than sd\_af as INFO decreases.
Using $\text{sd\_ss} / \sqrt{\text{INFO}}$ instead, we get a very good fit with sd\_af, except for some variants of chromosome 6 and 8. Most of these outlier variants are in regions 25-33 Mbp of chromosome 6 and 8-12 Mbp of chromosome 8 (Figure \ref{fig:hist-bad-pos}), which are two known long-range LD regions \cite[]{price2008long}.
We hypothesize that this is due to using principal components (PCs) that capture LD structure as covariates in GWAS \cite[]{prive2020efficient}.
To validate this hypothesis, we simulate a phenotype using HapMap3 variants of chromosome 6 for 10,000 individuals from the UK Biobank, then we run GWAS with or without PC19 as covariate. PC19 from the UK Biobank was previously reported to capture LD structure in region 70-91 Mbp of chromsome 6 \cite[]{prive2020efficient}.
In these simulations, the same bias as in figure \ref{fig:hist-bad-pos}B is observed for the variants in this region (Figure \ref{fig:gwas_bad_pc}), therefore confirming our hypothesis.
For the iCOGS summary statistics, we also see a clear trend with sd\_ss being lower and lower than sd\_af as INFO decreases, but they do not have issues with PCs  (Figure \ref{fig:brca_icogs_qc}).

[ALSO ADD THE GWAS META-ANALYSIS NEFF \textasciitilde20K?]

\begin{figure}[h]
	\centerline{\includegraphics[width=0.95\textwidth]{brca_onco_qc}}
	\caption{Standard deviations inferred from the OncoArray breast cancer GWAS summary statistics versus the ones inferred from the reported GWAS allele frequencies. Only 100,000 variants are represented, at random.}
	\label{fig:brca_onco_qc}
\end{figure}

Therefore, having the INFO score enables both to correct for it in the QC step to be able to pinpoint possible other problems, and possibly to correct for it and improve prediction as shown in the simulations before.
We try this for the two breast cancer summary statistics. We first compare the standard QC proposed in \cite{prive2020ldpred2} (which ends up filtering on MAF here, which we call ``qc1''). We then also filter out the two long-range LD regions of chromosome 6 and 8 for the OncoArray summary statistics and remove around 500 variants when filtering on differences of MAFs between summary statistics and the validation dataset.
As for the correction for the INFO scores, we use $\hat{\beta}_j^{imp} \cdot \text{INFO}_j$ and $N \cdot \text{INFO}_j$ as we have shown to perform well in simulations.
Although results for both QC used are very similar, correcting for the INFO score really improves prediction, especially for LDpred2-auto [TODO: NUMBERS] (Figure \ref{fig:res_brca}).

[SHOULD JUST REPORT ONE PREDICTION INSTEAD OF ALL? THEN MAKE A TABLE?]

\begin{figure}[h]
	\centerline{\includegraphics[width=0.95\textwidth]{res-brca}}
	\caption{AUC (for \textit{all} models) for predicting breast cancer in the UK Biobank when using either the OncoArray or the iCOGS summary statistics.}
	\label{fig:res_brca}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Discussion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{Materials and Methods}

\subsection*{Data for simulations}

We use the UK Biobank imputed (BGEN) data \cite[]{bycroft2018uk}. 
We restrict individuals to the ones used for computing the principal components (PCs) in the UK Biobank (Field 22020). These individuals are unrelated and have passed some quality control including removing samples with a missing rate on autosomes larger than 0.02, having a mismatch between inferred sex and self-reported sex, and outliers based on heterozygosity (more details can be found in section S3 of \cite{bycroft2018uk}).
To get a set of genetically homogeneous individuals, we compute a robust Mahalanobis distance based on the first 16 PCs and further restrict individuals to those within a log-distance of 5 \cite[]{prive2020efficient}.  
This results in 362,307 individuals.
We sample 300,000 individuals to form a training set (e.g.\ to run GWAS), 10,000 individuals to form a validation set (to tune hyper-parameters), and use the remaining 52,307 individuals to form a test set (to evaluate final predictive models).

Among genetic variants with a minor allele frequency larger than 0.01 and an imputation INFO score larger than 0.4 (as reported in the UK Biobank), we sample 40,000 of them according to the inverse of the INFO score density so that they have varying levels of imputation accuracy (Figure \ref{fig:hist-info}).
We read the UK Biobank data into two different datasets using function \texttt{snp\_readBGEN} from R package bigsnpr \cite[]{prive2017efficient}, one by reading the BGEN data at random according to imputation probabilities, and another one reading it as dosages (i.e.\ expected values according to imputation probabilities).
The first dataset is used as what could be the real genotype calls and the second dataset as what would be its imputed version; this technique was used in \cite{prive2019making}.

\subsection*{Data for real analyses}

We also use the UK Biobank data, and use the same individuals as described in the previous subsection. We sample 10,000 individuals to form a validation set and use the remaining 352,307 individuals as test set.
We restrict to the genetic variants to the 1,054,315 variants used in the LD reference provided in \cite{prive2020ldpred2}.

\subsection*{GWAS sample size imputation}

In this paper, we extensively use the following formula
\begin{equation}\label{eq:approx-sd-lin}
\text{sd}(G_j) \approx \dfrac{\text{sd}(y)}{\sqrt{n_j ~ \text{se}(\hat{\beta}_j)^2 + \hat{\beta}_j^2}} ~,
\end{equation}
where $\hat{\beta}_j$ is the marginal (GWAS) effect of variant $j$, $n_j$ is the GWAS sample size associated with variant $j$, $y$ is the vector of phenotypes and ${G}_j$ is the vector of genotypes for variant $j$.
This formula is used in LDpred2 \cite{prive2020ldpred2,prive2021high}. Note that, for a binary trait for which logistic regression is used, we have instead
\begin{equation}\label{eq:approx-sd-log}
\text{sd}(G_j) \approx \dfrac{2}{\sqrt{n_j^\text{eff} ~ \text{se}(\hat{\beta}_j)^2 + \hat{\beta}_j^2}} ~,
\end{equation}
where $n_j^\text{eff} = \frac{4}{1 / n_j^\text{cases} + 1 / n_j^\text{controls}}$.

We can then impute $n_j$ from equation \eqref{eq:approx-sd-lin} using
\begin{equation}\label{eq:imputeN}
n_j \approx \dfrac{\text{var}(y) / \text{var}(G_j) - \hat{\beta}_j^2}{\text{se}(\hat{\beta}_j)^2} ~.
\end{equation}
In practice, we also bound this estimate to be between $0.5 \cdot N$ and $1.1 \cdot N$, where $N$ is the global sample size.

\subsection*{Standard deviations and imputation measures}

The MACH $\hat{r}_j^2$ measure for variant $j$ is given by $\hat{r}_j^2 = \frac{var(G_j)}{2 \hat{\theta} (1 - \hat{\theta})}$, where $\hat{\theta}$ is the estimated allele frequency \cite[]{marchini2010genotype}.
This measure is also highly concordant with the IMPUTE INFO measure \cite[]{marchini2010genotype}.
Therefore $\text{INFO}_j \approx \frac{var(G_j)}{2 \hat{\theta} (1 - \hat{\theta})}$.

\subsection*{New implementation of lassosum in bigsnpr}

Instead of using a regularized version of the correlation matrix $R$ parameterized by $s$, $R_s = (1 - s) R + s I$, we use $R_{\delta} = R + \delta I$, which makes it clearer that lassosum is also using L2-regularization (therefore elastic-net). 
Then, from \cite{mak2017polygenic}, the solution from lassosum can be obtained by iteratively updating 
\[
\beta_j^{(t)} =
\begin{cases}
\text{sign}\left(u_j^{(t)}\right) \left(\left|u_j^{(t)}\right| - \lambda\right) / \left(\widetilde{X}_j^T \widetilde{X}_j + \delta\right) & \text{if } \left|u_j^{(t)}\right| > \lambda ~, \\
0 & \text{otherwise.}
\end{cases}
\]
where 
\[
u_j^{(t)} = r_j - \widetilde{X}_j^T \left( \widetilde{X} \beta^{(t-1)} - \widetilde{X}_j \beta_j^{(t-1)} \right) ~.
\]
Following notations from \cite{prive2020ldpred2} and denoting $\widetilde{X} = \frac{1}{\sqrt{n-1}} C_n G S^{-1}$, where $G$ is the genotype matrix, $C_n$ is the centering matrix and $S$ is the diagonal matrix of standard deviations of the columns of $G$.
Then $\widetilde{X}_j^T \widetilde{X} = R_{j,.} = R_{.,j}^T$, $\widetilde{X}_j^T \widetilde{X}_j = 1$ and
\[
u_j^{(t)} = \beta_j^{(t-1)} + \widehat{\beta}_j - R_{.,j}^T \beta^{(t-1)} ~,
\]
where $r_j = \widehat{\beta}_j =  \dfrac{\widehat{\beta}_j}{\sqrt{n_j ~ \text{se}(\widehat{\beta}_j)^2 + \widehat{\beta}_j^2}}$ and $\widehat{\beta}_j$ is the GWAS effect of variant $j$ and $n$ is the GWAS sample size \cite[]{mak2017polygenic,prive2021high}.
Then the most time-consuming part is computing $R_{.,j}^T \beta^{(t-1)}$.
To make this faster, instead of computing $R_{.,j}^T \beta^{(t-1)}$ at each iteration ($j$ and $t$), we can start with a vector with only 0s initially (for all $j$) since $\beta^{(0)} \equiv 0$, and then updating this vector when $\beta_j^{(t)} \neq \beta_j^{(t-1)}$ only. Note that only positions $k$ for which $R_{k,j} \neq 0$ must be updated in this vector $R_{.,j}^T \beta^{(t-1)}$. 

In this new implementation of the lassosum model, the input parameters are the correlation matrix $R$, the GWAS summary statistics ($\widehat{\beta}_j$, $\text{se}(\widehat{\beta}_j)$ and $n_j$), and the two hyper-parameters $\lambda$ and $\delta$. 
Therefore, except for the hyper-parameters, this is the exact same input as for LDpred2 \cite[]{prive2020ldpred2}.
We try $\delta \in \{0.001, 0.005, 0.02, 0.1, 0.6, 3\}$ by default in lassosum2, instead of $s \in \{0.2, 0.5, 0.8, 1.0\}$ in lassosum.
For $\lambda$, the default in lassosum uses a sequence of 20 values equally spaced on a log scale between 0.1 and 0.001; we now use a sequence between $\lambda_0$ and $\lambda_0 / 100$ by default in lassosum2, where $\lambda_0 = \max_j \left|\widehat{\beta}_j\right|$ is the minimum $\lambda$ for which no variable enters the model because the L1-regularization is too strong.
Note that we do not provide an ``auto'' version using pseudo-validation (as in \cite{mak2017polygenic}) as we have not found it to be very robust (Figure \ref{fig:pseudoval}).
Also note that, as in LDpred2, we run lassosum2 genome-wide using a sparse correlation matrix which assumes that variants further away than 3 cM are not correlated, and therefore we do not require splitting the genome into independent LD blocks anymore (as is required in lassosum).

\subsection*{New LD reference}

We make three changes to the LD reference. 
First, when using imputed data, we multiply the correlation between variants $j$ and $k$ by $\sqrt{\text{INFO}_j \cdot \text{INFO}_k}$ (for $j \neq k$) since it approximates well the correlation from non-imputed data (Figure \ref{fig:compare-cor}).
Second, we also define nearly independent LD blocks using the optimal algorithm developed in \cite{prive2021optimal}.
For different number of blocks and maximum number of variants in each block, we use the split with the minimum cost within the ones reducing the original number of non-zero values to less than 60\% (70\% for chromosome 6).
Having a correlation matrix with independent blocks prevents the small errors in the algorithm (e.g.\ the Gibbs sampler in LDpred2) from propagating to the whole genome.
It also makes running LDpred2 (and lassosum2) faster, taking about 60\% of the initial time (since only 60\% of the initial non-zero values of the correlation matrix are kept).
Finally, we have developed a new ``compact'' format for the SFBMs (sparse matrices on disk). Instead of using something similar to the standard ``compressed sparse column'' format which stores all $\{i,~x(i, j)\}$ for a given column $j$, we only store the first index $i_0$ and all the contiguous values $\{x(i_0, j),~x(i_0 + 1, j),~\dots\}$ up to the last non-zero value for this column $j$. This makes this format about twice as efficient for both LDpred2 and lassosum2.

\subsection*{LDpred2-low-h2 and LDpred2-auto-rob}

Here we introduce the small changes made to LDpred2 (-grid and -auto) in order to make them more robust.
First, LDpred2-low-h2 simply consists in running LDpred2-grid by testing $h^2$ within $\{0.3, 0.7, 1, 1.4\} \cdot h^2_\text{LDSC}$ (note the added $0.3$ compared to \cite{prive2020ldpred2}), where $h^2_\text{LDSC}$ is the heritability estimate from LD score regression. Indeed, we show in simulations here that using lower values for $h^2$ may provide higher predictive performance in the case of misspecifications (thanks to more shrinkage of the effects).
In simulations, because of the large misspecifications, we use a larger grid over $\{0.01, 0.1, 0.3, 0.7, 1, 1.4\} \cdot h^2_\text{LDSC}$.

For LDpred2-auto, we introduce two new parameters. The first one, \texttt{shrink\_corr}, allows for shrinking off-diagonal elements of the correlation matrix. This is similar to parameter `s' in lassosum, and act as a regularization. We use a value of 0.9 in simulations and 0.95 in real data when running ``LDpred2-auto-rob'' (and the default value of 1, without any effect, when running ``LDpred2-auto'').
The second new parameter, \texttt{allow\_jump\_sign}, controls whether variants can change sign over two consecutive iterations of the Gibbs sampler. When setting this parameter to false (in the method we name ``LDpred2-auto-rob'' here), this forces the effects to go through 0 before changing sign. This is useful to prevent instability (oscillation and ultimately divergence) of the Gibbs sampler under large misspecifications, and is also useful for accelerating convergence of chains with a large initial value for $p$, the proportion of causal variants.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\clearpage
%\vspace*{5em}

\section*{Code and results availability}

All code used for this paper is available at %\url{https://github.com/privefl/paper-lassosum2/tree/master/code}.
Latest versions of R package bigsnpr can be installed from GitHub.
A tutorial on running lassosum2 along with LDpred2 using R package bigsnpr is available at \url{https://privefl.github.io/bigsnpr-extdoc/polygenic-scores-pgs.html}.
We have extensively used R packages bigstatsr and bigsnpr \cite[]{prive2017efficient} for analyzing large genetic data, packages from the future framework \cite[]{bengtsson2020unifying} for easy scheduling and parallelization of analyses on the HPC cluster, and packages from the tidyverse suite \cite[]{wickham2019welcome} for shaping and visualizing results.

\section*{Acknowledgements}

Authors thank GenomeDK and Aarhus University for providing computational resources and support that contributed to these research results.
This research has been conducted using the UK Biobank Resource under Application Number 58024.

\section*{Funding}

F.P.\ and B.J.V.\ are supported by the Danish National Research Foundation (Niels Bohr Professorship to Prof. John McGrath).
B.J.V.\ is also supported by a Lundbeck Foundation Fellowship (R335-2019-2339).

\section*{Declaration of Interests}

The authors declare no competing interests.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\clearpage

\bibliographystyle{natbib}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
